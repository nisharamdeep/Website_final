---
title: "Project2"
author: "Nisha Ramdeep"
date: "11/26/2019"
output:
  pdf_document: default
  html_document: default
---



<div id="r-markdown" class="section level2">
<h2>R Markdown</h2>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>When you click the <strong>Knit</strong> button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:</p>
<pre class="r"><code>library(lmtest)</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre class="r"><code>library(sandwich)
library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## v ggplot2 3.2.1     v purrr   0.3.3
## v tibble  2.1.3     v dplyr   0.8.3
## v tidyr   1.0.0     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(carData)
data &lt;- GSSvocab
data1 &lt;- na.omit(data)
head(data1)</code></pre>
<pre><code>##        year gender nativeBorn ageGroup educGroup vocab age educ
## 1978.1 1978 female        yes    50-59    12 yrs    10  52   12
## 1978.2 1978 female        yes      60+   &lt;12 yrs     6  74    9
## 1978.3 1978   male        yes    30-39   &lt;12 yrs     4  35   10
## 1978.4 1978 female        yes    50-59    12 yrs     9  50   12
## 1978.5 1978 female        yes    40-49    12 yrs     6  41   12
## 1978.6 1978   male        yes    18-29    12 yrs     6  19   12</code></pre>
<pre class="r"><code>class_diag&lt;-function(probs,truth){
  
  tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord&lt;-order(probs, decreasing=TRUE)
  probs &lt;- probs[ord]; truth &lt;- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
  TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
  
  n &lt;- length(TPR)
  auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}</code></pre>
<pre class="r"><code>man1 &lt;- manova(cbind(vocab, educ)~gender, data=data1)
summary(man1)</code></pre>
<pre><code>##              Df    Pillai approx F num Df den Df    Pr(&gt;F)    
## gender        1 0.0037786   51.882      2  27357 &lt; 2.2e-16 ***
## Residuals 27358                                               
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man1)</code></pre>
<pre><code>##  Response vocab :
##                Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## gender          1     40  40.162  9.0731 0.002596 **
## Residuals   27358 121098   4.426                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response educ :
##                Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## gender          1    454  454.33  50.347 1.32e-12 ***
## Residuals   27358 246882    9.02                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>data1 %&gt;% group_by(gender) %&gt;% summarize(mean_vocab=mean(vocab, na.rm=T), mean_educ=mean(educ, na.rm=T))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   gender mean_vocab mean_educ
##   &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;
## 1 female       6.03      13.0
## 2 male         5.96      13.3</code></pre>
<pre class="r"><code>pairwise.t.test(data1$vocab, data1$gender, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data1$vocab and data1$gender 
## 
##      female
## male 0.0026
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(data1$educ, data1$gender, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  data1$educ and data1$gender 
## 
##      female 
## male 1.3e-12
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>1 - (0.95^5)</code></pre>
<pre><code>## [1] 0.2262191</code></pre>
<pre class="r"><code>0.05/5</code></pre>
<pre><code>## [1] 0.01</code></pre>
<p><em>The assumptions for a MANOVA are random samples, independent observations, multivariate normality of dependent variables, homogeneity of within-group covariance matrices, linear relationships between dependent variables, no extreme univariate or multivariate outliers, and no multicollinearity. These assumptions are difficult to test for and given that they are usually hard to meet, I would assume that they werenâ€™t met. Scores on a vocabulary test and education level (years of education) differed significantly across gender Pillai trace = 0.0038, pseudo F(2, 27470) = 51.882, p &lt; 0.001. Univariate analyses of variance were conducted as follow-ups, using the Bonferroni correction value of 0.01, the ANOVAs for vocab and educ were also significant with F(1, 27471) = 9.0731, p &lt; 0.01 and F(1, 27471) = 50.347, p &lt; 0.001 respectively. Post-hoc t-tests were conducted and males and females differed significantly from each other in terms of scores on the vocabulary test and education level even when using the Bonferroni correction value of 0.01. I performed 1 MANOVA, 2 ANOVAs, and 2 t-tests. The probability of type I error is 0.2262. The Bonferroni correction value, as mentioned previously, is 0.01.</em></p>
<pre class="r"><code>data1 %&gt;% group_by(nativeBorn) %&gt;% summarize(avg=mean(vocab)) %&gt;% summarize(mean_diff=diff(avg))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   mean_diff
##       &lt;dbl&gt;
## 1     0.937</code></pre>
<pre class="r"><code>rand_dist&lt;-vector()

for(i in 1:5000){
  new &lt;- data.frame(score=sample(data1$vocab), origin=data1$nativeBorn)
  rand_dist[i] &lt;- mean(new[new$origin == &quot;yes&quot;,]$score) - 
    mean(new[new$origin == &quot;no&quot;,]$score)
}

mean(rand_dist &gt; 0.937)*2</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>{hist(rand_dist,main=&quot;&quot;,ylab=&quot;&quot;, xlim=c(-0.20,0.95)); abline(v = 0.937, col=&quot;red&quot;)}</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><em>Null hypothesis: the mean vocabulary test scores are the same for those born in the U.S. and those not born in the U.S. Alternative hypothesis: the mean vocabulary test scores are different for those not born in the U.S. and those not born in the U.S. The mean vocabulary scores differ significantly for those born in the U.S. and those not born in the U.S. (p = 0).</em></p>
<pre class="r"><code>#general linear model
data1$age_c &lt;- data1$age-mean(data1$age)
fit1 &lt;- lm(vocab~gender*age_c, data=data1)
summary(fit1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = vocab ~ gender * age_c, data = data1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.3481 -1.1025  0.0428  1.1395  4.1743 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       6.0290457  0.0168763 357.249  &lt; 2e-16 ***
## gendermale       -0.0690399  0.0256524  -2.691  0.00712 ** 
## age_c             0.0073757  0.0009507   7.759 8.89e-15 ***
## gendermale:age_c -0.0019492  0.0014872  -1.311  0.18998    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.101 on 27356 degrees of freedom
## Multiple R-squared:  0.003345,   Adjusted R-squared:  0.003236 
## F-statistic:  30.6 on 3 and 27356 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#plot
data1 %&gt;% ggplot(aes(x = age, y = vocab, fill = gender)) +
  geom_smooth(method=&quot;lm&quot;, aes(color=gender))</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>#check assumptions
resids &lt;- fit1$residuals
fitvals &lt;- fit1$fitted.values
ggplot() + geom_point(aes(fitvals, resids)) + geom_hline(yintercept=0, col=&quot;red&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code>bptest(fit1)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit1
## BP = 164.04, df = 3, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot() + geom_histogram(aes(resids), bins=20)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<pre class="r"><code>#robust standard errors
coeftest(fit1, vcov = vcovHC(fit1))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                     Estimate  Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       6.02904566  0.01665744 361.9431 &lt; 2.2e-16 ***
## gendermale       -0.06903986  0.02574381  -2.6818  0.007327 ** 
## age_c             0.00737572  0.00097901   7.5339 5.078e-14 ***
## gendermale:age_c -0.00194921  0.00152812  -1.2756  0.202122    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#rerun without interaction
fit2 &lt;- lm(vocab~gender+age_c, data=data1)
summary(fit2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = vocab ~ gender + age_c, data = data1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.3141 -1.1073  0.0375  1.1427  4.2019 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.0295012  0.0168730 357.347  &lt; 2e-16 ***
## gendermale  -0.0686322  0.0256508  -2.676  0.00746 ** 
## age_c        0.0065792  0.0007311   8.999  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.101 on 27357 degrees of freedom
## Multiple R-squared:  0.003282,   Adjusted R-squared:  0.003209 
## F-statistic: 45.04 on 2 and 27357 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><em>For participants with an average age, malesâ€™ scores are 0.0690399 points lower than females (t=-2.691, df=27356, p&lt;0.05). For females, with every 1 year increase in age, the vocabulary test scores increased by 0.0073757 points (t=7.759, p&lt;0.05). The effect of age on vocabulary test scores is the same for males and females (t=-1.31, p&gt;0.05). The assumptions were not met. The t-statistics obtained from the the robust standard errors are lower than the ones obtained using the simple linear regression. The p-values for both the gendermale and age_c were significant for both the simple linear regression and the heteroskedastic robust standard errors. The proportion of variance explained (using the adjusted R-squared) is 0.003236. Both gender and age_c continue to be significant even when the model is run without the interaction.</em></p>
<pre class="r"><code>samp_distn &lt;- replicate(5000, {
  boot_dat &lt;- data1[sample(nrow(data1),replace=TRUE),]
  fit&lt;-lm(vocab~gender*age, data=boot_dat)
  coef(fit)
})

samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>##   (Intercept) gendermale          age gendermale:age
## 1  0.04608086 0.07055204 0.0009703655    0.001510185</code></pre>
<pre class="r"><code>samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% gather %&gt;% group_by(key) %&gt;%
  summarize(lower=quantile(value,.025), upper=quantile(value,.975))</code></pre>
<pre><code>## # A tibble: 4 x 3
##   key               lower    upper
##   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)     5.60    5.78    
## 2 age             0.00546 0.00927 
## 3 gendermale     -0.116   0.159   
## 4 gendermale:age -0.00489 0.000948</code></pre>
<p><em>The standard errors are slightly larger for age and for the interaction but a lot larger for the gendermale coefficient. As the SEs increase, the t-values decrease and the p-values increase.</em></p>
<pre class="r"><code>#logistic regression
data2 &lt;- data1 %&gt;% mutate(x=ifelse(nativeBorn == &quot;yes&quot;,1,0))
fit3 &lt;- glm(x~educGroup+vocab, data=data2, family=binomial(link=&quot;logit&quot;))
summary(fit3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = x ~ educGroup + vocab, family = binomial(link = &quot;logit&quot;), 
##     data = data2)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8649   0.3108   0.3749   0.4536   1.1686  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         0.95439    0.06112  15.616  &lt; 2e-16 ***
## educGroup12 yrs     0.42996    0.06618   6.497 8.19e-11 ***
## educGroup13-15 yrs -0.08365    0.06631  -1.261    0.207    
## educGroup16 yrs    -0.49660    0.07807  -6.361 2.00e-10 ***
## educGroup&gt;16 yrs   -0.93357    0.08074 -11.563  &lt; 2e-16 ***
## vocab               0.27028    0.01144  23.623  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 15991  on 27359  degrees of freedom
## Residual deviance: 15242  on 27354  degrees of freedom
## AIC: 15254
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>exp(coef(fit3))</code></pre>
<pre><code>##        (Intercept)    educGroup12 yrs educGroup13-15 yrs    educGroup16 yrs 
##          2.5970930          1.5372001          0.9197572          0.6085935 
##   educGroup&gt;16 yrs              vocab 
##          0.3931465          1.3103260</code></pre>
<pre class="r"><code>#CV
prob &lt;- predict(fit3, data=data2, type=&quot;response&quot;)
truth &lt;- data2$x
class_diag(prob,truth)</code></pre>
<pre><code>##         acc sens spec       ppv       auc
## 1 0.9144006    1    0 0.9144006 0.6494416</code></pre>
<pre class="r"><code>#confusion matrix
table(truth = data2$x, predicted = (prob &gt; 0.5)) %&gt;% addmargins</code></pre>
<pre><code>##      predicted
## truth  TRUE   Sum
##   0    2342  2342
##   1   25018 25018
##   Sum 27360 27360</code></pre>
<pre class="r"><code>#density plot
data2$logit &lt;- predict(fit3)
data2 %&gt;% ggplot(aes(logit, fill=gender)) + geom_density(alpha=0.3) + geom_vline(xintercept= 0, lty=2)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>#ROCplot
library(plotROC)
data4 &lt;- data2 %&gt;% mutate(prob = predict(fit3, type=&quot;response&quot;), prediction = ifelse(prob &gt; 0.5, 1, 0))
classify &lt;- data4 %&gt;% transmute(prob, prediction, truth = x)

ROCplot &lt;- ggplot(classify) + geom_roc(aes(d = truth, m = prob), n.cuts = 0)
ROCplot</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.6494416</code></pre>
<pre class="r"><code>#10-fold CV
set.seed(1234)
k = 10

data3 &lt;- data2[sample(nrow(data2)), ] #randomly order rows
folds &lt;- cut(seq(1:nrow(data2)), breaks = k, labels = F) #create folds
diags &lt;- NULL

for (i in 1:k) {
  ## Create training and test sets
  train &lt;- data3[folds != i, ]
  test &lt;- data3[folds == i, ]
  truth &lt;- test$x
  
  ## Train model on training set
  fit &lt;- glm(x ~ educGroup+vocab, data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)

  ## Test model on test set (save all k results)
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

apply(diags, 2, mean)</code></pre>
<pre><code>##         acc        sens        spec         ppv         auc 
## 0.914473684 0.999959726 0.001259224 0.914497335 0.649078256</code></pre>
<p><em>For a person with education &lt;12 years, with every 1 point increase in vocabulary test scores, the odds of being born in the U.S. increase by a factor of 1.310. The odds of being born in the U.S. if you were in school for 12 years are 1.537 times that of the odds that you were born in the U.S. if you only spent &lt;12 years in school. The odds of being born in the U.S. if you were in school for 13-15 years are 0.920 times that of the odds that you were born in the U.S. if you only spent &lt;12 years in school. The odds of being born in the U.S. if you were in school for 16 years are 0.609 times that of the odds that you were born in the U.S. if you only spent &lt;12 years in school. The odds of being born in the U.S. if you were in school for &gt;16 years are 0.393 times that of the odds that you were born in the U.S. if you only spent &lt;12 years in school. The accuracy is 0.914, sensitivity is 1, specificity is 0, and the ppv is 0.914. The AUC is 0.649, which is poor. This means that it is hard to predict whether you were born in the U.S. based off the vocabulary test scores and years spent in school. The accuracy is 0.914, sensitivity is 1, specificity is 0, and ppv is 0.914.</em></p>
<pre class="r"><code>#lasso regression
library(glmnet)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 3.0-2</code></pre>
<pre class="r"><code>head(data2)</code></pre>
<pre><code>##   year gender nativeBorn ageGroup educGroup vocab age educ      age_c x
## 1 1978 female        yes    50-59    12 yrs    10  52   12   6.254569 1
## 2 1978 female        yes      60+   &lt;12 yrs     6  74    9  28.254569 1
## 3 1978   male        yes    30-39   &lt;12 yrs     4  35   10 -10.745431 1
## 4 1978 female        yes    50-59    12 yrs     9  50   12   4.254569 1
## 5 1978 female        yes    40-49    12 yrs     6  41   12  -4.745431 1
## 6 1978   male        yes    18-29    12 yrs     6  19   12 -26.745431 1
##      logit
## 1 4.087115
## 2 2.576048
## 3 2.035497
## 4 3.816839
## 5 3.006011
## 6 3.006011</code></pre>
<pre class="r"><code>data7 &lt;- data2 %&gt;% select(-age_c, -age, -educ, -year)
fit4 &lt;- glm(x~gender+ageGroup+educGroup+vocab, data=data7, family=binomial(link=&quot;logit&quot;))
head(data7)</code></pre>
<pre><code>##   gender nativeBorn ageGroup educGroup vocab x    logit
## 1 female        yes    50-59    12 yrs    10 1 4.087115
## 2 female        yes      60+   &lt;12 yrs     6 1 2.576048
## 3   male        yes    30-39   &lt;12 yrs     4 1 2.035497
## 4 female        yes    50-59    12 yrs     9 1 3.816839
## 5 female        yes    40-49    12 yrs     6 1 3.006011
## 6   male        yes    18-29    12 yrs     6 1 3.006011</code></pre>
<pre class="r"><code>y &lt;- model.matrix(fit4)
z &lt;- as.matrix(data2$x)

cv &lt;- cv.glmnet(y, z, family=&quot;binomial&quot;)
lasso &lt;- glmnet(y,z, family=&quot;binomial&quot;, lambda=cv$lambda.1se)
coef(cv)</code></pre>
<pre><code>## 12 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                              1
## (Intercept)         1.33599525
## (Intercept)         .         
## gendermale          .         
## ageGroup30-39       .         
## ageGroup40-49       .         
## ageGroup50-59       .         
## ageGroup60+         0.09999175
## educGroup12 yrs     0.28797937
## educGroup13-15 yrs  .         
## educGroup16 yrs     .         
## educGroup&gt;16 yrs   -0.33498966
## vocab               0.16915863</code></pre>
<pre class="r"><code>#10 fold CV
data9 &lt;- data7 %&gt;% mutate(s=ifelse(ageGroup==&quot;60+&quot;,1,0), t=ifelse(educGroup==&quot;12 yrs&quot;,1,0), e=ifelse(educGroup==&quot;&gt;16 yrs&quot;,1,0))
data9 &lt;- data9 %&gt;% select(vocab, s, t, e, x)
set.seed(1234)
k = 10

data8 &lt;- data9[sample(nrow(data9)),]
folds2 &lt;- cut(seq(1:nrow(data9)), breaks=k, labels=F)

diags2 &lt;- NULL
for (i in 1:k) {
  ## Create training and test sets
  train &lt;- data8[folds != i, ]
  test &lt;- data8[folds == i, ]
  truth &lt;- test$x
  
  ## Train model on training set
  fit &lt;- glm(x~. , data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)

  ## Test model on test set (save all k results)
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

apply(diags, 2, mean)</code></pre>
<pre><code>##         acc        sens        spec         ppv         auc 
## 0.914437135 0.999979863 0.000629612 0.914448960 0.651295860</code></pre>
<p><em>The variables retained from the LASSO are ageGroup60+, educGroup12 yrs, educGroup&gt;16 yrs, and vocab. The accuracy is 0.914, sensitivity is 1, specificity is 0, and ppv is 0.914. These values are the same when compared to the 10-fold CV done in part 5. The AUC is 0.651, which is slightly greater than the one from part 5. This shows that using the most significant predictors does improve the model.</em></p>
</div>
